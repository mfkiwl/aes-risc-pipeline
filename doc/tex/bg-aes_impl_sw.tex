% =============================================================================

In a software-only implementation,
execution of AES
and
the associated application program
is 
performed by 
a general-purpose processor core, using only instructions in the base ISA.
Since we only consider use of the RISC-V scalar base ISA, we exclude work on
the use of vector-like extensions~\cite{Hamburg:09}.

Software-only techniques are important because 
many ISEs are evaluated against baseline ISA implementations.
Work such as that of
Bernstein and Schwabe~\cite{BerSch:08},
Osvik et al.~\cite{OBSC:10},
and
Schwabe and Stoffelen~\cite{SchSto:16}
present and compare multiple techniques across a range of platforms, but,
for completeness, we present a (limited) survey in what follows.

% -----------------------------------------------------------------------------

\paragraph{Compute-oriented.}

A compute-oriented implementation of AES favours
 online     computation, 
thus reducing 
memory footprint
at the cost of increased 
latency.
Following~\cite[Section 4.1]{DaeRij:02}, for example, the idea is to simply
1) adopt an
    array-packed
   representation of state and round key matrices,
   then
2) construct a round implementation by following the algorithmic description
   of each round function in a direct manner.
Addition in $\F_{2^8}$ can be implemented with a base ISA XOR instruction.
Base ISA support is rarely present for multiplication and inversion in
$\F_{2^8}$ however.
Hence it is common to pre-compute the \ALG{S-box} and/or \AESFUNC{xtime} 
functions.
This requires pre-computation and storage of a
$
\SI{256}{\byte}
$
look-up table per function, but significantly reduces execution latency.

On platforms where $R = 32$,
Bertoni et al.~\cite{BBFMM:02}
improve execution latency by exploiting the wider data-path.
They adopt a row-packed representation of state and round key matrices,
implementing
\AESFUNC{ShiftRows}
using native rotation instructions to act on the packed rows.
\AESFUNC{MixColumns} is implemented
using the SIMD Within A Register (SWAR) paradigm:
applying
\AESFUNC{xtime}
across a packed row in parallel.

% -----------------------------------------------------------------------------

\paragraph  {Table-oriented.}

A  table-oriented implementation of AES favours
offline pre-computation,
reducing 
latency
but increasing the
memory footprint.
The main example of this technique is the so-called
T-tables~\cite[Section 4.2]{DaeRij:02} method.
This involves
adopting a 
column-packed
representation of state and round key matrices and
pre-computing
   $
   \AESFUNC{MixColumn} \circ \AESFUNC{SubBytes}
   $
   using the tables
   \[
   \begin{array}{cc}
   \begin{array}{lcl}
   T_0[x] &=& \left[\begin{array}{c}
                    \RADIX{02}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{01}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{01}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{03}{16} \AESMUL \ALG{S-box}( x ) \\
                    \end{array} \right]
   \end{array}
   &
   \begin{array}{lcl}
   T_1[x] &=& \left[\begin{array}{c}
                    \RADIX{03}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{02}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{01}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{01}{16} \AESMUL \ALG{S-box}( x ) \\
                    \end{array} \right]
   \end{array}
   \\\\
   \begin{array}{lcl}
   T_2[x] &=& \left[\begin{array}{c}
                    \RADIX{01}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{03}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{02}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{01}{16} \AESMUL \ALG{S-box}( x ) \\
                    \end{array} \right]                 
   \end{array}
   &
   \begin{array}{lcl}
   T_3[x] &=& \left[\begin{array}{c}
                    \RADIX{01}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{01}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{03}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{02}{16} \AESMUL \ALG{S-box}( x ) \\
                    \end{array} \right]
   \end{array}
   \end{array}
   \]
   for $x \in \F_{2^8}$,
   then computing each $j$-th column of $\AESRND{s}{r+1}$ as
   \[
   T_0[ \AESRND{s}{r}_{i, j + i \pmod{Nb}} ] \AESADD
   T_1[ \AESRND{s}{r}_{i, j + i \pmod{Nb}} ] \AESADD
   T_2[ \AESRND{s}{r}_{i, j + i \pmod{Nb}} ] \AESADD
   T_3[ \AESRND{s}{r}_{i, j + i \pmod{Nb}} ]
   \]
   where extraction of elements caters for \AESFUNC{ShiftRows}, then XOR'ing 
   the $j$-th column of $\AESRND{rk}{r}$ to cater for \AESFUNC{AddRoundKey}.

As such, each round becomes a sequence of look-ups into $T_i$, plus XORs 
to combine their result.
Doing so demands pre-computation and storage of a
$
256 \cdot \SI{4}{\byte} = \SI{1}{\kilo\byte}
$
look-up table per $T_i$.
The overhead related to extraction of each element from 
packed columns representing $\AESRND{s}{r}$ 
(to form look-table offsets) 
can be significant:
Fiskiran and Lee~\cite{FisLee:01}
analyse the impact of different addressing modes on this issue, with
Stoffelen~\cite[Section 3.1]{Stoffelen:19}
concluding that RISC-V is ill-equipped to reduce said overhead,
due to the provision of a sparse set of addressing modes.
Further, in systems with data caches, T-table based implementations are
susceptible to timing attacks \cite{DJB:05}.

% -----------------------------------------------------------------------------

\paragraph{Bit-sliced.}

The term bit-slicing is an implementation technique due to
Biham~\cite{Biham:97},
which constitutes

\begin{enumerate}
\item a non-standard {\em representation}
      of data where
      each $R$-bit word $x$ is transformed into $\REP{x}$,
      i.e.,
      $R$ slices, say $\REP{x}[ i ]$ for
      $
      0 \leq i < R ,
      $
      where $\REP{x}[ i ]_j = x_i$ for some $j$,
      and
\item a non-standard {\em implementation}
      of operation:
      each operation $f$ used as
      $
          {r} =     {f}(     {x} )
      $
      must be transformed into a ``software circuit'' $\REP{f}$,
      i.e.,
      a sequence of Boolean instructions acting on the slices st.
      $
      \REP{r} = \REP{f}( \REP{x} ) .
      $
\end{enumerate}

\noindent
Bit-slicing introduces some overhead related to conversion of $x$ into
$\REP{x}$ and $\REP{r}$ into $r$, plus the (relative) inefficiency
of $\REP{f}$ vs. $f$ wrt. latency and footprint.
However, if each slice is itself an $R$-bit word, then it
is possible to compute $R$ instances of $\REP{f}$ in {\em parallel}
on suitably packed $\REP{x}$.
A common analogy is that of transforming the 
$R$-bit, $1$-way scalar processor 
into a 
$1$-bit, $R$-way SIMD   processor, 
thus giving (or recouping) up to a $R$-fold improvement in latency.

As evidenced by \cite{MatNak:07,Konighofer:08} and \cite{KasSch:09},
the application of bit-slicing to AES can be very effective;
Stoffelen~\cite[Section 3.1]{Stoffelen:19}
specifically investigates this fact within the context of RISC-V.

% =============================================================================
